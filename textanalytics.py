# -*- coding: utf-8 -*-
"""TextAnalytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TKTTp-p99eCVg9Mai3MwzW6Y8ONVS-co

The process you've outlined is a good high-level overview of the typical steps involved in text analysis as a data science professional. Let me provide some more details on each step:

1. **Gather the Text Data:** In this initial step, you collect textual data from various sources. This could include web scraping, accessing APIs, or using pre-existing datasets. Ensure that you have a diverse and representative dataset for the problem you're trying to solve.

2. **Clean and Preprocess the Text Data:** Raw text data usually contains noise, such as HTML tags, special characters, or irrelevant information. Text cleaning involves tasks like removing HTML tags, punctuation, and stopwords (common words like "the," "and," "is" that don't provide much information). You may also perform tasks like lowercasing and stemming/lemmatization to normalize the text.

3. **Convert Text into Numerical Format:** Machine learning models require numerical input. Common techniques for converting text to numerical format include TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and word embeddings like Word2Vec or GloVe. These methods represent words or phrases as numerical vectors.

4. **Analyze Text Data to Gain Insights:** Exploratory Data Analysis (EDA) is essential to understand the characteristics of your text data. You can generate word clouds, frequency distributions, or visualize the data in other ways to identify patterns, trends, and potential issues.

5. **Create Relevant Features:** Depending on your analysis goals, you might need to engineer additional features from the text data. For example, you can calculate sentiment scores, create bag-of-words or n-grams representations, or perform topic modeling to extract topics or themes from the text.

6. **Select Appropriate NLP Models:** Choosing the right NLP models depends on your specific task. For sentiment analysis, you might use pre-trained models like BERT or LSTM-based models. For named entity recognition, you can use Conditional Random Fields or deep learning models. Topic modeling often involves techniques like Latent Dirichlet Allocation (LDA) or Non-Negative Matrix Factorization (NMF). Text classification can be done using a variety of algorithms, including Naive Bayes, SVM, or deep learning models.

7. **Evaluate and Iterate:** Once you've built your models, you need to evaluate their performance. Use appropriate metrics like accuracy, precision, recall, F1-score, or others, depending on your task. Iterate on your models and data preprocessing steps to improve performance.

8. **Deployment and Visualization:** If your goal is to create a practical application, deploy your model in a production environment. Visualize the results to communicate insights effectively, using techniques like word clouds, bar charts, or interactive dashboards.
"""

import pandas as pd
import plotly.express as px
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from textblob import TextBlob
import spacy
from collections import defaultdict
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

nlp = spacy.load('en_core_web_sm')

data = pd.read_csv("/content/drive/MyDrive/kaggle datasets/articles.csv", encoding='latin-1')
print(data.head())

# Combine all titles into a single string
titles_text = ' '.join(data['Title'])

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(titles_text)

# Plot the Word Cloud
fig = px.imshow(wordcloud, title='Word Cloud of Titles')
fig.update_layout(showlegend=False)
fig.show()

# Sentiment Analysis
data['Sentiment'] = data['Article'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Sentiment Distribution
fig = px.histogram(data, x='Sentiment', title='Sentiment Distribution')
fig.show()

# NER
def extract_named_entities(text):
    doc = nlp(text)
    entities = defaultdict(list)
    for ent in doc.ents:
        entities[ent.label_].append(ent.text)
    return dict(entities)

data['Named_Entities'] = data['Article'].apply(extract_named_entities)

# Visualize NER
entity_counts = Counter(entity for entities in data['Named_Entities'] for entity in entities)
entity_df = pd.DataFrame.from_dict(entity_counts, orient='index').reset_index()
entity_df.columns = ['Entity', 'Count']

fig = px.bar(entity_df.head(10), x='Entity', y='Count', title='Top 10 Named Entities')
fig.show()

# Topic Modeling
vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')
tf = vectorizer.fit_transform(data['Article'])
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_topic_matrix = lda_model.fit_transform(tf)

# Visualize topics
topic_names = ["Topic " + str(i) for i in range(lda_model.n_components)]
data['Dominant_Topic'] = [topic_names[i] for i in lda_topic_matrix.argmax(axis=1)]

fig = px.bar(data['Dominant_Topic'].value_counts().reset_index(), x='index', y='Dominant_Topic', title='Topic Distribution')
fig.show()