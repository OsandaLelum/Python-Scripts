# -*- coding: utf-8 -*-
"""web scraping to extract data from a website.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WSBw_7yBFqdvfpd61C1pHIg1ZszH7j6v
"""

import requests
from bs4 import BeautifulSoup

def scrape_data(url):
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Now, you can extract data from the 'soup' object
        # For example, let's extract all the text within <p> tags
        paragraphs = soup.find_all('p')

        # Print the text from all the <p> tags
        for paragraph in paragraphs:
            print(paragraph.text)
    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)

# Example usage:
url_to_scrape = 'https://osandal.github.io/'
scrape_data(url_to_scrape)

import requests
from bs4 import BeautifulSoup

def scrape_data(url):
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Now, you can extract data from the 'soup' object
        # For example, let's extract all the text within <p> tags
        paragraphs = soup.find_all('p')

        # Print the text from all the <p> tags
        for paragraph in paragraphs:
            print(paragraph.text)
    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)

# Example usage:
url_to_scrape = 'https://github.com'
scrape_data(url_to_scrape)

import requests
from bs4 import BeautifulSoup

def scrape_data(url):
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        # Parse the HTML content of the page using BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')

        # Now, you can extract data from the 'soup' object
        # For example, let's extract all the text within <p> tags
        paragraphs = soup.find_all('p')

        # Print the text from all the <p> tags
        for paragraph in paragraphs:
            print(paragraph.text)
    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)

# Example usage:
url_to_scrape = 'https://gritires.com'
scrape_data(url_to_scrape)

import requests
from bs4 import BeautifulSoup
import os

# Function to download and save an image from a URL
def download_image(image_url, save_path):
    response = requests.get(image_url)

    if response.status_code == 200:
        with open(save_path, 'wb') as file:
            file.write(response.content)
    else:
        print(f"Failed to download image from {image_url}")

# Function to scrape images from a webpage
def scrape_images(url, output_directory):
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all image tags in the HTML
        img_tags = soup.find_all('img')

        # Create the output directory if it doesn't exist
        if not os.path.exists(output_directory):
            os.makedirs(output_directory)

        # Download and save each image
        for img_tag in img_tags:
            img_url = img_tag.get('src')

            # Ensure that the URL is valid and not a data URI or empty
            if img_url and not img_url.startswith('data:') and not img_url.startswith('#'):
                img_name = os.path.basename(img_url)
                img_save_path = os.path.join(output_directory, img_name)

                download_image(img_url, img_save_path)
                print(f"Downloaded: {img_url}")

    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)

# Example usage:
url_to_scrape = 'https://osandal.github.io/assets/img/o.gif'
output_directory = '/content/sample_data'

scrape_images(url_to_scrape, output_directory)

https://www.novacura.com/